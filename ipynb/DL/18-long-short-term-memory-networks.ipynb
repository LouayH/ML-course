{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long short-term memory (LSTM) networks\n",
    "\n",
    "LSTM is a type of RNN that can detain long-term dependencies in sequence data, and use a memory cell to control the flow of information.\n",
    "\n",
    "<u>Memory cell gates</u>\n",
    "\n",
    "* Input gate decides which information to store in the memory cell. It is trained to open when the input is important and close when it is not.\n",
    "* Forget gate decides which information to discard from the memory cell. It is trained to open when the information is no longer important and close when it is. \n",
    "* Output gate is responsible for deciding which information to use for the output of the LSTM. It is trained to open when the information is important and close when it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep 1000 most frequent words, replace the less frequent with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = 1000, oov_char = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 [1, 14, 22, 16, 43, 530, 973, 0, 0, 65, 458, 0, 66, 0, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 0, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 0, 336, 385, 39, 4, 172, 0, 0, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 0, 19, 14, 22, 4, 0, 0, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 0, 4, 22, 17, 515, 17, 12, 16, 626, 18, 0, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 0, 0, 16, 480, 66, 0, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 0, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 0, 8, 4, 107, 117, 0, 15, 256, 4, 0, 7, 0, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 0, 0, 13, 104, 88, 4, 381, 15, 297, 98, 32, 0, 56, 26, 141, 6, 194, 0, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 0, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 0, 88, 12, 16, 283, 5, 16, 0, 113, 103, 32, 15, 16, 0, 19, 178, 32] 1\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[0]), x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2494)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# infinty\n",
    "min_length = float(\"inf\")\n",
    "max_length = 0\n",
    "\n",
    "for sequence in x_train:\n",
    "  sequence_length = len(sequence)\n",
    "  min_length = min(min_length, sequence_length)\n",
    "  max_length = max(max_length, sequence_length)\n",
    "\n",
    "min_length, max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad reviews with length < 512 with 0s at the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_length = 512\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen = max_sequence_length)\n",
    "x_test = pad_sequences(x_test, maxlen = max_sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embeddings\n",
    "\n",
    "Embeddings are mathematical form (vectors) representations of values or objects like text, images, and audio that are designed to be consumed by machine learning models.\n",
    "\n",
    "They allow models to find similar objects like photos or documents, and make it possible to understand the relationship between words or other objects\n",
    "\n",
    "For example, documents near each other in an embedding may be relevent to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding** layer will convert the every word in the sequence into a numeric array (vector)\n",
    "\n",
    "* input_dim: unique words count\n",
    "* output_dim: vector size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Input\n",
    "\n",
    "# num_words = 1000\n",
    "input_dim = 1000\n",
    "output_dim = 128\n",
    "\n",
    "model = Sequential([\n",
    "  Input(shape = (max_sequence_length, )),\n",
    "  Embedding(input_dim = input_dim, output_dim = output_dim),\n",
    "  LSTM(128),\n",
    "  Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 480ms/step - accuracy: 0.6738 - loss: 0.5986\n",
      "Epoch 2/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 429ms/step - accuracy: 0.8038 - loss: 0.4372\n",
      "Epoch 3/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 478ms/step - accuracy: 0.7866 - loss: 0.4495\n",
      "Epoch 4/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 586ms/step - accuracy: 0.8777 - loss: 0.2908\n",
      "Epoch 5/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 447ms/step - accuracy: 0.8902 - loss: 0.2714\n",
      "Epoch 6/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 559ms/step - accuracy: 0.8965 - loss: 0.2578\n",
      "Epoch 7/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 573ms/step - accuracy: 0.9068 - loss: 0.2304\n",
      "Epoch 8/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m421s\u001b[0m 539ms/step - accuracy: 0.9154 - loss: 0.2172\n",
      "Epoch 9/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 598ms/step - accuracy: 0.9199 - loss: 0.2033\n",
      "Epoch 10/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 429ms/step - accuracy: 0.9291 - loss: 0.1787\n",
      "Epoch 11/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m331s\u001b[0m 423ms/step - accuracy: 0.9369 - loss: 0.1651\n",
      "Epoch 12/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m393s\u001b[0m 436ms/step - accuracy: 0.9409 - loss: 0.1555\n",
      "Epoch 13/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 426ms/step - accuracy: 0.9490 - loss: 0.1374\n",
      "Epoch 14/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 426ms/step - accuracy: 0.9541 - loss: 0.1241\n",
      "Epoch 15/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 436ms/step - accuracy: 0.9632 - loss: 0.1057\n",
      "Epoch 16/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 440ms/step - accuracy: 0.9618 - loss: 0.1022\n",
      "Epoch 17/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 429ms/step - accuracy: 0.9725 - loss: 0.0769\n",
      "Epoch 18/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 422ms/step - accuracy: 0.9762 - loss: 0.0717\n",
      "Epoch 19/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 428ms/step - accuracy: 0.9817 - loss: 0.0578\n",
      "Epoch 20/20\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 436ms/step - accuracy: 0.9833 - loss: 0.0523\n"
     ]
    }
   ],
   "source": [
    "model_output = model.fit(x_train, y_train, batch_size = 32, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 193ms/step - accuracy: 0.8605 - loss: 0.5990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5712215900421143, 0.8644800186157227)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"18-dumps/model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_review(review):\n",
    "  review_words = review.lower().split()\n",
    "\n",
    "  # the first 3 indexes are reserved in the imdb dataset\n",
    "  # for example: \"the\" is the most frequent word, but actually its index is 4\n",
    "  review = [word_index.get(word, 0) + 3 if word in word_index else 0 for word in review_words]\n",
    "  review = [x if x <= 1000 else 0 for x in review]\n",
    "  review = pad_sequences([review], maxlen = max_sequence_length)\n",
    "\n",
    "  return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(\"18-dumps/model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(review):\n",
    "  review = process_review(review)\n",
    "\n",
    "  predictions = model.predict(review)\n",
    "\n",
    "  return \"Positive\" if predictions[0] > 0.5 else \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Negative sentiment: The food was fantastic\n",
      "Negative sentiment: The movie was terrible\n",
      "Positive sentiment: I love this place, it is a great one\n"
     ]
    }
   ],
   "source": [
    "sample_data = [\n",
    "  \"The food was fantastic\",\n",
    "  \"The movie was terrible\",\n",
    "  \"I love this place, it is a great one\"\n",
    "]\n",
    "\n",
    "predictions = [predict_sentiment(review) for review in sample_data]\n",
    "\n",
    "for review, prediction in zip(sample_data, predictions):\n",
    "  print(f\"{prediction} sentiment: {review}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
